---
apiVersion: optimize.stormforge.io/v1beta2
kind: Experiment
metadata:
  name: kafka-experiment-001
  namespace: kafka
  labels:
    stormforge.io/application: 'kafka'
    stormforge.io/scenario: 'perftest'

spec:
  optimization:
  - name: experimentBudget
    value: "60"

  # Ideas:
  # - Ideal heap size
  #   Never saw benefit above 6GB
  #   512MB heap was typical, rest for OS/FS page cache
  #   Makes a difference using larger message sizes for clients.
  # - Storage and iops are usually the defining performance factors
  # - Topics:
  #      - partition count
  #      - replica count
  #      - Affected by whether data can be separated by key
  #      - Consumer: try and demonstrate fetch-from-follower improvement?
  #      - 
  parameters:
  - name: partitions
    baseline: 30
    min: 1
    max: 3000
  - name: replicas
    baseline: 1
    min: 1
    max: 3
  - name: heap_mb
    baseline: 1024
    min: 256
    max: 8192

  metrics:
  - name: consumer-MBs-per-second
    type: prometheus
    query: 'scalar( consumer_mbs_per_second {job="trialRun", instance="{{ .Trial.Name }}"} )'
  - name: consumer-msgs-per-second
    type: prometheus
    query: 'scalar( consumer_msgs_per_second {job="trialRun", instance="{{ .Trial.Name }}"} )'
  - name: producer-msgs-per-second
    type: prometheus
    query: 'scalar( producer_msgs_per_second {job="trialRun", instance="{{ .Trial.Name }}"} )'
  - name: producer-avg-ms-latency
    type: prometheus
    query: 'scalar( producer_avg_ms_latency {job="trialRun", instance="{{ .Trial.Name }}"} )'
  - name: producer-p95-ms-latency
    type: prometheus
    query: 'scalar( producer_p95_ms_latency {job="trialRun", instance="{{ .Trial.Name }}"} )'

  patches:
  - targetRef:
      apiVersion: apps/v1
      kind: StatefulSet
      name: kafka
    patch: |
      spec:
        template:
          spec:
            containers:
            - name: kafka
              resources:
                requests:
                  memory: {{ .Values.heap_mb }}Mi
              env:
              - name: KAFKA_HEAP_OPTS
                value: '-Xmx{{ .Values.heap_mb }}m -Xms{{ .Values.heap_mb }}m'

  trialTemplate:
    spec:
      setupServiceAccountName: optimize-pro-setuptask-sa
      setupTasks:
      - name: monitoring
        args:
        - prometheus
        - $(MODE)

      jobTemplate:
        spec:
          parallelism: 2
          completions: 2
          template:
            spec:
              restartPolicy: Never
              volumes:
              - name: work-dir
                emptyDir: {}
              initContainers:
              - name: topic-creator
                image: docker.io/bitnami/kafka:3.3.1
                resources:
                  requests:
                    memory: "100Mi"
                    cpu: "100m"
                command:
                - '/bin/sh'
                - '-c'
                - |
                  kafka-topics.sh --create --if-not-exists \
                    --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 \
                    --topic "test-${PARTITIONS}x${REPLICAS}" \
                    --partitions $PARTITIONS \
                    --replication-factor $REPLICAS \
                    --config retention.ms=300000
                  exit 0
              containers:
              - name: consumer
                image: docker.io/bitnami/kafka:3.3.1
                resources:
                  requests:
                    memory: "1Gi"
                    cpu: "500m"
                volumeMounts:
                - mountPath: /work-dir
                  name: work-dir
                command:
                - '/bin/sh'
                - '-c'
                - |
                  set -e
                  kafka-consumer-perf-test.sh \
                    --bootstrap-server kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 \
                    --topic "test-${PARTITIONS}x${REPLICAS}" \
                    --messages 1000000 \
                  | tee /work-dir/consumer.out
                  curl --data-binary @- "$PUSHGATEWAY_URL" <<EOF
                    consumer_mbs_per_second $(tail -1 /work-dir/consumer.out | awk -F ', ' '{print $4}')
                    consumer_msgs_per_second $(tail -1 /work-dir/consumer.out  | awk -F ', ' '{print $6}')
                  EOF
              - name: producer
                image: docker.io/bitnami/kafka:3.3.1
                resources:
                  requests:
                    memory: "500Mi"
                    cpu: "500m"
                env:
                - name: PARTITIONS
                  value: "30"
                - name: REPLICAS
                  value: "3"
                volumeMounts:
                - mountPath: /work-dir
                  name: work-dir
                command:
                - '/bin/sh'
                - '-c'
                - |
                  set -e
                  kafka-producer-perf-test.sh \
                    --topic "test-${PARTITIONS}x${REPLICAS}" \
                    --num-records 1000000 \
                    --throughput -1 \
                    --record-size 1000 \
                    --producer-props \
                        bootstrap.servers=kafka-0.kafka-svc:9092,kafka-1.kafka-svc:9092,kafka-2.kafka-svc:9092 \
                        batch.size=1000 \
                        acks=1 \
                        linger.ms=100000 \
                        buffer.memory=268435456 \
                        compression.type=none \
                        request.timeout.ms=300000 \
                  | tee /work-dir/producer.out
                  curl --data-binary @- "$PUSHGATEWAY_URL" <<EOF
                    producer_msgs_per_second $(tail -1 /work-dir/producer.out | cut -f 4 -d ' ')
                    producer_avg_ms_latency $(tail -1 /work-dir/producer.out  | cut -f 8 -d ' ')
                    producer_p95_ms_latency $(tail -1 /work-dir/producer.out  | cut -f 19 -d ' ')
                  EOF
